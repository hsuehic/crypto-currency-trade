# Time Series Query Optimization

## üö® Problem

Account analytics API was experiencing **critical performance issues**:

```
QueryFailedError: canceling statement due to statement timeout
query is slow: SELECT ... FROM "account_snapshots" ... 
execution time: 7053ms (timeout at 10000ms)
```

**Symptoms:**
- ‚ùå API timeouts (7-21+ seconds)
- ‚ùå Database query timeouts
- ‚ùå Loading 10,000+ rows into memory
- ‚ùå Poor user experience

## üîç Root Cause

The `getBalanceTimeSeries()` method was using an **inefficient approach**:

### Before (Inefficient)

```typescript
async getBalanceTimeSeries(...) {
  // Step 1: Load ALL rows into memory (10,000+ rows!)
  const snapshots = await this.getHistory(exchange, startTime, endTime);
  
  // Step 2: Process in memory with JavaScript
  if (interval === 'hour' || snapshots.length <= 100) {
    return snapshots.map(...); // Return all 10,000+ points
  }
  
  // Step 3: Group by day/week in JavaScript
  const groupedData: Map<string, AccountSnapshotData[]> = new Map();
  snapshots.forEach((snapshot) => {
    // JavaScript processing of 10,000+ rows
  });
  
  // Step 4: Return aggregated data
  return result;
}
```

**Problems:**
1. ‚ùå Loads ALL rows (10,000+) from database
2. ‚ùå Processes aggregation in JavaScript (slow)
3. ‚ùå High memory usage
4. ‚ùå Query takes 7+ seconds
5. ‚ùå Hits statement timeout (10s)

### Query Performance

```sql
-- Before: Loads ALL rows
SELECT * FROM account_snapshots 
WHERE exchange = 'okx' 
  AND timestamp BETWEEN '2025-09-11' AND '2025-10-11'
ORDER BY timestamp ASC;
-- Result: 10,718 rows, 7+ seconds ‚ùå
```

## ‚úÖ Solution: Database-Level Aggregation

Use PostgreSQL's `DISTINCT ON` with `date_trunc` to **downsample at the database level**.

### After (Optimized)

```typescript
async getBalanceTimeSeries(...) {
  // Determine sampling interval
  let truncInterval: string;
  let maxPoints = 500; // Limit data points for chart
  
  switch (interval) {
    case 'hour': truncInterval = 'hour'; maxPoints = 24 * 7; break;
    case 'week': truncInterval = 'week'; maxPoints = 52; break;
    case 'day': default: truncInterval = 'day'; maxPoints = 90; break;
  }
  
  // Use database-level aggregation with DISTINCT ON
  const rawResults = await this.repository.query(
    `
    SELECT DISTINCT ON (date_trunc($1, timestamp)) 
      timestamp, 
      "totalBalance"
    FROM account_snapshots
    WHERE exchange = $2 
      AND timestamp BETWEEN $3 AND $4
    ORDER BY date_trunc($1, timestamp), timestamp DESC
    LIMIT $5
    `,
    [truncInterval, exchange, startTime, endTime, maxPoints]
  );
  
  // Map to result (already aggregated)
  return rawResults.map((row: any) => ({
    timestamp: new Date(row.timestamp),
    balance: new Decimal(row.totalBalance),
  }));
}
```

**Advantages:**
1. ‚úÖ Database does the aggregation (PostgreSQL is optimized for this)
2. ‚úÖ Only loads necessary data points (max 500 instead of 10,000+)
3. ‚úÖ Uses indexes efficiently
4. ‚úÖ **70,000x faster** (7s ‚Üí 0.1ms)
5. ‚úÖ Low memory usage

### Optimized Query Performance

```sql
-- After: Downsamples at database level
SELECT DISTINCT ON (date_trunc('day', timestamp)) 
  timestamp, 
  "totalBalance"
FROM account_snapshots
WHERE exchange = 'okx' 
  AND timestamp BETWEEN '2025-09-11' AND '2025-10-11'
ORDER BY date_trunc('day', timestamp), timestamp DESC
LIMIT 90;
-- Result: ~30 rows, 0.1ms ‚úÖ
```

**EXPLAIN ANALYZE Results:**
```
Execution Time: 0.101 ms
Index Scan using "IDX_f9941d6827dffe9f1d9c52c156"
```

## üìä Performance Comparison

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Query Time** | 7,053ms | 0.1ms | **70,000x faster** ‚ö° |
| **API Response** | 7-21 seconds | 51ms | **137-411x faster** üöÄ |
| **Rows Loaded** | 10,718 | ~30 | **357x less data** üìâ |
| **Memory Usage** | High (10,000+ objects) | Low (~30 objects) | **357x less** üíö |
| **Timeout Errors** | Yes ‚ùå | None ‚úÖ | **100% resolved** |

## üéØ How DISTINCT ON Works

### PostgreSQL DISTINCT ON

```sql
SELECT DISTINCT ON (date_trunc('day', timestamp)) 
  timestamp, 
  "totalBalance"
FROM account_snapshots
WHERE exchange = 'okx' AND timestamp BETWEEN '...' AND '...'
ORDER BY date_trunc('day', timestamp), timestamp DESC
LIMIT 90;
```

**What it does:**
1. Groups rows by `date_trunc('day', timestamp)` (e.g., "2025-10-11 00:00:00")
2. For each group, keeps only the **first row** (determined by ORDER BY)
3. Since we `ORDER BY ... timestamp DESC`, it keeps the **latest snapshot per day**
4. Returns only the selected columns (`timestamp`, `totalBalance`)
5. Limits result to 90 rows (90 days of data)

**Result:**
- From 10,000+ snapshots ‚Üí ~30 data points (one per day)
- Perfect for chart rendering
- Instant query execution

## üõ†Ô∏è Technical Details

### Why This Works

1. **Indexes are used**: The composite index `(exchange, timestamp)` allows fast filtering
2. **Minimal data transfer**: Only 2 columns √ó 30 rows instead of 13 columns √ó 10,000 rows
3. **Database-native**: PostgreSQL is highly optimized for this operation
4. **Appropriate granularity**: Charts don't need 10,000 data points

### Downsampling Strategy

```typescript
switch (interval) {
  case 'hour':
    // For short time periods (1h-7d), show hourly data
    truncInterval = 'hour';
    maxPoints = 24 * 7; // Max 168 data points
    break;
    
  case 'day':
    // For medium periods (7d-90d), show daily data
    truncInterval = 'day';
    maxPoints = 90; // Max 90 data points
    break;
    
  case 'week':
    // For long periods (90d+), show weekly data
    truncInterval = 'week';
    maxPoints = 52; // Max 52 data points (1 year)
    break;
}
```

**Chart Rendering Considerations:**
- Most charts display 300-1000 pixels width
- More than 500 data points is wasteful
- Human eye can't distinguish 10,000 points
- Downsampling improves UX

## üß™ Testing Results

### Before Optimization

```bash
$ time curl "http://localhost:3000/api/analytics/account?exchange=okx&period=30d"
# Error: QueryFailedError: canceling statement due to statement timeout
# Time: 10+ seconds (timeout) ‚ùå
```

### After Optimization

```bash
$ time curl "http://localhost:3000/api/analytics/account?exchange=okx&period=30d"
{
  "summary": {
    "totalBalance": 13000.458257,
    "totalEquity": 13000.458257,
    ...
  },
  "chartData": [30 data points]
}
# Time: 0.051 seconds (51ms) ‚úÖ
```

## üìù Files Changed

1. **`packages/data-manager/src/repositories/AccountSnapshotRepository.ts`**
   - Optimized `getBalanceTimeSeries()` method
   - Changed from in-memory aggregation to database-level aggregation
   - Added `DISTINCT ON` query with `date_trunc`

## üéâ Results

‚úÖ **No more timeout errors**  
‚úÖ **70,000x faster database queries**  
‚úÖ **137-411x faster API responses**  
‚úÖ **357x less data loaded**  
‚úÖ **Smooth user experience**  
‚úÖ **Scalable to millions of rows**  

## üí° Key Lessons

### 1. **Let the Database Do the Work**

```typescript
// ‚ùå Bad: Load all data and process in JavaScript
const allData = await db.query('SELECT * FROM table');
const aggregated = allData.reduce(...); // Slow!

// ‚úÖ Good: Aggregate at database level
const aggregated = await db.query('SELECT date_trunc(...), AVG(...)');
```

### 2. **Downsample for Time Series**

```typescript
// ‚ùå Bad: Return all 10,000 data points
return allSnapshots;

// ‚úÖ Good: Return appropriate granularity
return downsampledSnapshots; // Max 500 points
```

### 3. **Use Database Features**

- PostgreSQL `date_trunc()` - Time-based grouping
- `DISTINCT ON` - Get first row per group
- `LIMIT` - Prevent runaway queries
- Indexes - Enable fast filtering

### 4. **Monitor Query Performance**

```typescript
// Enable slow query logging
maxQueryExecutionTime: 5000 // Log queries > 5s
```

## üîß Future Optimizations

If data grows even larger:

1. **Materialized Views**
   ```sql
   CREATE MATERIALIZED VIEW daily_account_snapshots AS
   SELECT DISTINCT ON (date_trunc('day', timestamp), exchange)
     exchange, timestamp, "totalBalance"
   FROM account_snapshots
   ORDER BY date_trunc('day', timestamp), exchange, timestamp DESC;
   ```

2. **Partitioning**
   ```sql
   -- Partition by month for faster queries
   CREATE TABLE account_snapshots_2025_10 PARTITION OF account_snapshots
   FOR VALUES FROM ('2025-10-01') TO ('2025-11-01');
   ```

3. **Background Aggregation**
   - Pre-compute daily/weekly aggregates
   - Store in separate table
   - Update via cron job

4. **Redis Caching**
   - Cache aggregated time series data
   - 5-minute TTL
   - Invalidate on new snapshot

## üìö Resources

- [PostgreSQL DISTINCT ON](https://www.postgresql.org/docs/current/sql-select.html#SQL-DISTINCT)
- [PostgreSQL date_trunc](https://www.postgresql.org/docs/current/functions-datetime.html)
- [Time Series Database Design](https://www.timescale.com/blog/time-series-data-postgresql-10-tips/)
- [Query Optimization Best Practices](https://wiki.postgresql.org/wiki/Performance_Optimization)

---

**The timeout error is completely resolved! API now responds in 51ms instead of timing out at 10+ seconds.** üéâ‚ú®

---

**Author**: xiaoweihsueh@gmail.com  
**Date**: October 11, 2025

